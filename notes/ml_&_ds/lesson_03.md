{% include libs/mathjax.html %}

# Машинное обучение и анализ данных
## Метрические методы классификации и регрессии

<iframe width="560" height="315" src="https://www.youtube.com/embed/GyOxB2itxnc" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>


## Конспект
### [00:00:00 - Ретроспектива пройденных тем](https://youtu.be/GyOxB2itxnc?t=0)

**Парадигмы машинного обучения:**
* **Минимизация эмпирического риска** - сведение задачи машинного обучения к оптимизация. Это общий подход, который объединяет, наверно, практически все методы машинного обучения. Если говорить о **классификации**, то этот подход приводит к **принципу разделимости**. То есть, чтобы классифицировать мы строим в явном виде разделяющую поверхность. В частности она может быть линейной или нелинейной
* **Нейронные сети** - они как бы логично вытекают из предыдущей, потому что **линейный классификатор** - это модель нейрона. Но **нейроны** можно объединять друг за другом и строить **нейронные сети**
* Еще одна парадигма основана на понятии **близости между объектами**

### [00:01:42 - Содержание лекции](https://youtu.be/GyOxB2itxnc)

### [00:02:22 - Гипотезы непрерывности и компактности](https://youtu.be/GyOxB2itxnc?t=142)

Иногда эту гипотезу называют гипотезой **непрерывности** или **компактности** 

**X** - объекты, **Y** - ответы

$$X^l=(x_i,y_i)^l_{i=0}$$ - обучающая выборка

**Нам нужно построить алгоритм**, который по объекту предсказывает ответ. Рассмотрим две задачи:  **регрессию** и **классификацию**


#### **Регрессия**
В **регрессии** мы очень привыкли к тому, что
мы **восстанавливаем непрерывные или гладкие функциональные зависимости**

**Гипотеза непрерывности (для регрессии):** близким объектам соответствуют близкие значения

**Когда гипотеза непрерывности не выполнена**:
* нам как-то очень трудно провести функцию через заданные точки
* мы не можем увидеть некую модель или некий закон природы

**Когда гипотеза непрерывности выполнена:**
* можно увидеть некую зависимость, иногда даже глазами

**Гипотеза о непрерывности восстанавливаемой функции** дает на самом деле очень существенную подсказку о том, как решать задачу 

#### **Классификация**
Аналогом **гипотезы непрерывности** в классификации является **гипотеза компактности**

**Гипотеза компактности (для классификации):** близкие объекты, как правило, лежат в одном классе

По другому эту гипотезу можно сформулировать так:
**разделяющая поверхность** между классами проходит не повсюду, что она компактно расположена в пространстве - классы образуют компактные сгустки точек или, по другому, границы между классами компактно расположены в пространстве

**Если гипотеза компактности не выполнена**, то очень трудно провести **разделяющую поверхность**, то есть как будто бы вот эти поверхности они повсюду расположены, классы взаимопроникают друг другу. Тогда кажется задачу классификация вообще решить невозможно и, наверное, решать ее бессмысленно

### [00:05:20 - Пример: задача классификации цветков ириса [Фишер, 1936]](https://youtu.be/GyOxB2itxnc?t=320)

Привычная мера близости - **евклидова метрика** $$R^2$$

**Евклидово расстояние между точками:** оно позволяет сказать, что близкие точки по этому расстоянию - они в одном классе, далекие точки - скорее всего в разных классах

### [00:06:08 - Формализация понятия "близости"](https://youtu.be/GyOxB2itxnc?t=368)

**Евклидова метрика** - это сумма квадратов отклонений признаков даынных двух объектов под корнем:
$$\rho(x,x_i)=(\sum_{j=1}^n|x^j-x_i^j|^2)^{1/2}$$

Эта конструкция очень легко обобщается по двум направлениям:
1. Квадрат можно заменить на произвольную степень **p**
2. Имеет смысл разные признаки брать с разными весами, то есть водить веса признаков $$w_i$$. Это прежде всего связано с тем, что признаки могут измеряться в разных масштабах. Если один признак измеряется сотых долях обычно, а другой признак в миллионах, то понятно, что их надо как-то отмасштабировать иначе второй признак будет в метрике настолько превалировать, что первого признака вы там не увидите; то есть метрика будет фактически не по двум признакам, а по одному 

Данное обобщение - это **обобщённая метрика Минковского:**
$$\rho(x,x_i)=(\sum_{j=1}^nw_i|x^j-x_i^j|^p)^{1/p}$$

**Экви-дистантные поверхности** - это геометрическое место точек, лежащих на одинаковом расстоянии от центра от заданной точки. Окружность соответствуют евклидовой метрики
 остальные вот


### [00:07:51 - Расстояние между строками/сигналами](https://youtu.be/GyOxB2itxnc?t=472)

Метрики можно водить даже тогда, когда у нету признаковых описаний объектов. Это задачи, в которых ввести признаки сложнее, чем в явном виде ввести **функцию расстояния между объектами** 

**Редакторское расстояние Левенштейна (для строк)** - это количество вставок, удалений и замен, которое одну троку переводит в другую строку

**Расстояние для сигналов** - это энергия сжатий и растяжений.Например, по тройке кривых (координаты горизонтальная и вертикальная, мгновенный азимут движения пера) решается
задача распознавания рукописных надписей: человек пишет электронным пером на планшете и требуется распознавать эту надпись побуквенно 

Пример того, как можно сравнить две буквы которые немножко похожи друг на друга **d** и **a**. На 6-ом слайде видно, что здесь в некотором смысле аналог редакторского расстояния: здесь есть какие-то локальные сжатия и растяжения, с помощью которых можно 1 пучок траекторий наложить на другой пучок траектории. Энергия этих сжатий и растяжений может быть использована как **функция расстояния** между двумя пучками траектории 

### [00:10:50 - Расстояние между изображениями](https://youtu.be/GyOxB2itxnc?t=650)

Один из подходов к тому, чтобы мерить **расстояние между изображениями** - когда на изображение накладывается некая сетка и ставится задачу поиска минимальные искажений, при которых, накладывая сетку двух изображений друг на друга вы можно добиться их совпадений. **Энергия растяжения** этой сетки тоже может быть использована как **функция расстояния между объектами**


### [00:11:22 - Обобщенный метрический классификатор](https://youtu.be/GyOxB2itxnc?t=682)

Есть множество таких задач, где **определить расстояние между объектами проще, чем синтезировать какое-то признаковое описание** для этих объектов. Для таких задач, когда нету признаков, но есть способ измерять расстояние, как раз и можно использовать **метрические алгоритмы**

Когда стоит задача классифицировать некий объект $$x \in X$$ и есть объекты обучающей выборки $$x_1, ..., x_l$$, то самое простое, что можно сделать - это отранжировать объект обучающей выборки по возрастанию расстояний:
$$\rho(x,x^{(1)})\leqslant\rho(x,x^{(2)})\leqslant...\leqslant\rho(x,x^{(l)})$$
 
тогда можно обозначить:
$$x^{(i)}$$ - i-ый сосед объекта *x* среди $$x_1, ..., x_l$$;
$$y^{(i)}$$ - ответ на i-ом соседе объекта *x*

**Вариационный ряд** - это ряд чисел, который упорядоченный по возрастанию или убыванию

Общий **алгоритм метрического классификатора**, который сначала оценивает на сколько объект *x* близок к объектам класса *y*:
$$a(x, X^l)=\arg \max_{y\in Y} \underbrace{\sum_{i=1}^l[y^(i)=y]w(i, x)}_{Г_y(x)}$$

вот он наш объект и упорядочиваются

причем важно что относительно к

конкретного объекта x поэтому x

скобочках ли ты то этой сосед объекты x

серия обучающих объектов ну а y это это

соответственно ответ на этом объекте и

вот пользуясь этими обозначениями легко

записать общую конструкцию метрического

классификатора который сначала оценивает

насколько объект x близок к объектам

класса y и вот эта оценка близости будет

обозначаться гамма y от x что это такое

сумма по всем объектам класса y да вот

это вот условия она нам говорит о том

что мы суммируем только по объектам

класса y а вот здесь вот у нас некий

весовой коэффициент или степень важности

этого соседа объекта x и вот сейчас мы

займемся тем что рассмотрим разные

частные случаи которые будут отличаться

тем как задается вот этот вес

ну а дальше веса или степени важности

соседей раз суммировались

по всем объектам данного класса y и мы

причисляем объект x к тому классу y для

которого вот это вот суммарная оценка

близости максимально то есть сколько

соседей было в окрестности объекта x

каждого из классов то есть какой класс

оказался в большинстве в этой

окрестности

значит тому классу и надо относить x то

есть это непосредственная формализация

гипотезы компактности и при этом мы

используем только функцию расстояния

между объектами но давайте посмотрим на

частные случаи

как эта конструкция может выглядеть

метод ближайшего соседа дубль вы от и x

да это вес этого соседа для

классификации объекта x только первый

сосед имеет вес единицу остальные все

соседи просто не учитываются то есть

относи объект к тому классу которому

принадлежит его ближайший сосед

чем это плохо но тем что случайно может

каким-то причинам объект

1 класса попасть в сгусток объектов

других классов тогда и он будет

классифицироваться неправильно и

соседний с ним будут классифицироваться

неправильно и надежность классификации

можно повысить

если смотреть не на 1 ближайшего соседа

а на несколько ближайших соседей

причем возникает параметр водка

ближайших соседей как выбрать этот самый

параметров сколько ближайших соседей

здесь на помощь приходят критерий

лив one out это критерии скользящего

контроля который мы с вами так в сколь

на 1 лет ты упоминали

и мы о нем будем довольно часто

вспоминать как он устроен мы берем для

каждого объекта x это все объекты

обучающей выборке кроме него самого и

оцениваем правда ли что по ближайшему

окружению объекта x это и мы

действительно можем правильно

классифицировать этот самый объект x это

вот и очень просто выглядит этот

критерий и вот метод ближайшего соседа

вот

хорош тем что он безумно простой

наверное это самый вообще простой метод

классификации которые только существуют

и поскольку здесь нет никакой

оптимизации нет никакой настройки

параметров вот пока что кроме этого

самого к такое обучение называли

называют ленивым обучение майонезе

лирник то есть нам ничего не надо делать

по для обучения кроме того что сохранить

обучающую выборку и даже чтобы лифанова

считается да ничего не надо делать кроме

того что выкидывать из обучающей выборке

сам объект который мы сейчас пытаемся

классифицировать все вот такой вот очень

ленивая расслабленно и

очень приятный метод почти ничего не

надо делать ну вот маленькая иллюстрация

к тому насколько важно действительно

сделать лифанов

то есть насколько важно сам объект

выкинуть из обучающей выборке кота когда

мы пытаемся на нем сделать классификацию

вот синяя кривая здесь показывает а что

произошло бы если бы мы этого не сделали

мы бы получили бы смещенную оптимистично

заниженную оценку

числа ошибок и это мы можем называть в

полном праве

называть это переобучение да то здесь

нам показалось что значит что на этом

графике изображена по горизонтальной оси

число соседи отложено по вертикальной

оси частота ошибок

синяя линия это если сам объект

учитывается в своей окрестности то есть

среди ближайших соседей объекта есть он

сам но и понятно что если мы окрестность

берем маленькую то мы получаем ну как бы

0 ошибок в то время как честная

классификация

объекта по его ближайшему соседу это

порядка восьми процентов ошибок то есть

мы тут вот искусственно

искусственно занизили ошибку то есть

надо пользоваться lifan out если не

выкинуть сам объект из его окресности то

мы получим совершенно неправильную

неправильный вывод о том что здесь надо

пользоваться методом 1 ближайшего соседа

на самом деле надо пользоваться методом

там 30 ближайших соседей а еще лучше 60

ближайших соседей вот то есть и еще один

интересный момент вот на этой кривой

видно что действительно очень широк

диапазон

оптимального количества ближайших

соседей

параметр к здесь вот на графике немножко

неправильно на россии подписано q я на

предыдущем слайде обозначал через к

к ближайших соседей но вот видно что

очень широк диапазон от 30 до 7

и это довольно часто так происходит что

выбор вот этого параметра числа

ближайших соседей он не очень критичен

то есть существует широкий диапазон где

мы получаем примерно оптимальные

значения можно немножечко обобщить метод

ближайших соседей и сказать так а если

первый сосед а он должен иметь бою желез

чем второй сосед

2 больше чем 3 то есть кажется что

vesa соседей разумно чтобы убывали

с ростом номер соседа чем дальше объект

расположен тем меньше он должен влиять

на классификацию данного соседа вопрос

как задать вот эти веса ну приходят в

голову несколько простых эвристик

например давайте мы сделаем эти веса

линейно убывающими на самом деле это не

очень хороший выбор по уж представьте

себе когда

1 4 сосед лежат в одном классе а второй

и третий сосед лежат в другом классе и

тогда у средняя вот эти линейные веса

вывод для 2 классов получить одинаковые

оценки и вы не сможете с помощью арк

макса принять решение о том к кому же из

этих двух классов отнести объект его

чтобы таких случайных совпадений не

происходило без а нужно задать

убывающими по какому-то не линейному

закону ну например по экспоненте

и вот такой способ задания лесов он

решает эту проблему но опять-таки

возникает другой вопрос хорошо допустим

у нас три первых соседа они близко лежат

к объекту а четвертый сосед очень далеко

а мы пользуемся методом 4 ближайших

соседей и тогда вот этот четвертый с тем

же как бы почему почему вес объекта

зависит от номера его в ранжированных

списке они от расстояния наверное было

бы правильнее если бы мы вели бы

зависимость от расстояния до

действительно так можно сделать и это

приводит нас к очень известному методу

который называется метод портновского

окна или окна пар зона здесь наш

коэффициент весовой коэффициент

этого соседа объекта x устроен следующим

образом это убывающая функция к

от расстояния между x и его и там

соседом вот эту вот функция крана

называется ядро она не возрастает

положительно ну вот разные бывают

варианты задания функции к

иногда и и задают на отрезке 01 так

называемый финита ядра а за пределами

отрезка оно равно нулю

иногда можно задать эту функцию которая

вот глотку бывает от 0 до плюс

бесконечности я сейчас покажу какие они

могут быть эти функции ядра

на следующих слайдах ну вот если мы

подставим вот эту вот весовую функцию в

нашу общую конструкцию метрического

классификатора то мы получим вот такую

вот штуковину и у нас фактически вместо

параметра к ближайших соседей появляется

другой параметр это ширина окна

вот сколько объектов попадают в это окно

по этим объектам

мы принимаем решение о том какому классу

отнести

наш x и если например я дура обладает

свойством что она больше нуля только на

отрезке 0 1 то фактически арш нам просто

покажет радиус окресности в которой мы

смотрим на точке то есть все точки

которые вне радиуса аж от центра x

находятся они как бы для

нас не существует в чем опасность такого

подхода в том что объект x может

оказаться далеко вообще от любых

объектах обучающей выборке на расстояние

больше аж и тогда мы не сможем

классифицировать этот объект и что

делать в этом случае но два решения вас

можно либо не пользоваться fi нет на

меня не кедрами а использовать только и

др которые имеют значение больше нуля от

0 до плюс бесконечности один подход а

второй подход это окно порно переменной

ширины то есть мы говорим о пусть аж на

самом деле определяется так чтобы у нас

в это окно всегда к соседи попала что мы

для этого должны сделать а взять

расстояние для как + 1 соседа в качестве

аж вот значит k + 1 сосед будет уже

иметь нулевое значение ядра а предыдущий

к соседей к ближайших к x объектов

обучающей выборке они получат какие-то

не нулевые значения весовых

коэффициентов и все у нас будет хорошо и

вот этот параметр либо к либо аж в

зависимости от того какую из этих двух

формул мы используем они точно так же

могут быть оптимизированы по критерию

left one а вот пример давайте посмотрим

глазами что тут происходит когда мы

задали выборку из двух классов зеленые

точки 1 класс красные точки другой класс

и я взял

здесь не написано взято здесь по моему

было экспоненциальное ядро неважно самом

деле какое ядро но важно что вот на этой

картиночке очень маленькое взяли ширину

окна

что значит очень маленькая ширина окна

это значит что для классификации

практически всегда используется

ближайший сосед и действительно если вы

посмотрите на вот эту вот разделяющую

поверхность приглядитесь внимательнее

как оно проходит то вы увидите что она

очень похожа на кусочно линейную

поверхность причем каждый отрезочек

этой линии он связан с тем что мы вот

проходим перпендикуляром посередине

между двумя объектами разных классов вот

каждому прямолинейному участку 1 пример

вот этого прямолинейно участок ему

соответствует 1 объект красный и вот это

объект зеленый вот эту часть вот объект

зеленый вот обед красный то есть каждая

такая линия она определена как какой-то

парой точек двух разных классов но

кое-где линия размазана это потому что я

на самом деле градиентом цвета показываю

здесь значение вот этой вот разности у

меня 2 классовая выборка и

арк макс от гаммы y может быть

эквивалентным образом переписан как знак

разности и вот если разность

положительно мы относим объект

положительному классу отрицательно к

отрицательному класса есть я взял вот

эту вот разность и ее значение отложил

градиентом вот этого цвета фактически

зеленое это то что относится к классу +

1 красная классу минус 1 вот давайте аж

начнем увеличивать вот было 005 обратить

внимание на масштаб -24 -24 да то есть

выборка вся находится в квадратике 6 на

6 и 005 это вот в масштабах этой выборке

это очень мало это фактически мы

описываем очень небольшую окрестность

вокруг объекта 002 окрестность

увеличивается и здесь у нас уже

больше соседей захватываются вот этой

вот окрестностью и разделяющую

поверхность перестает быть похожей на

кусочно-линейной

она уже становится все более и более

гладкой мы увеличиваем ширину окна вот

она еще больше 0 0 теперь 05

да она уже достаточно большая чтобы вот

у нас зеленая

выборка она вот так вот выделялась на

фоне красной на самом деле это модельные

данные и как их сгенерировал я взял 2 га

ущербны одну с очень большой дисперсии

вторую с маленькой дисперсии и как будто

слегка наложил их друг на друга вот и

получился такой вот зеленый класс внутри

классно красного класса

ну и вот если дальше увеличивать

вот этот вот параметр то видно что

разделяющая поверхность упрощает свою

форму но и в пределе если мы ширину окна

начинаем стримить бесконечности на самом

деле метод ближайших соседей начинает

стремиться к линейному классификатору

вот такой вот интересный факт то есть

есть некая связь между линейными

разделяющими поверхностями методом

ближайшего соседа наиболее сложная

изрезанная

разделяющая поверхность получается в том

случае если мы берем мало ближайших

соседей или маленькую окрестность

маленького радиуса

еще одно обобщение давайте воспользуемся

вот такое физическая аналогия вот

смотрите что мы сделали когда мы ввели

метод пар зову ского окна

мы сказали что у нас есть объекте

который нам надо классифицировать и

теперь давайте посмотрим на его

окрестность и давайте посмотрим сколько

объектов разных классов лежат в этой

окрестности но функция расстояния между

объектами она же функция симметричная

поэтому мы можем использовать

двойственную идеологию и сказать а

давайте теперь мы поставим как бы центр

вселенной по очереди в каждый объект

обучающей выборке него объект который мы

собираемся классифицировать не vx а в x

5 и скажем так вот каждый объект

распространяет вокруг себя ну что-то

вроде потенциала своего класса

что говорить про электро статический

потенциал но как бы в электростатике

только два класса да у нас

задачам нога классовые ну давайте тогда

говорить о том что каждый объект

распространяет вокруг себя запах своего

класса вот можно так например сказать и

тогда если мы хотим классифицировать

объекты x в произвольной точке

пространства мы находимся то мы хотим

сказать вот заряда какого класса здесь

больше или запаха какого класса здесь

больше в этой точке и вот эта идея она

на самом деле очень похожа на предыдущую

но в отличие от нет да пар за нас кого

окна здесь появляется новая заманчивая

возможность когда мы говорим что каждый

объект распространяет вокруг себя

запах своего класса мы можем сказать что

во-первых

сила этого запаха до или концентрация

у каждого объекта своя и во-вторых

радиус

на который этот запах распространяется

тоже свой для каждого объекта и у нас

появляется возможность вести два

параметра для каждого объекта обучающей

выборке мы можем вести параметр

амплитуды вот такой гамма и можем вести

параметр radius а вот такой вот

аж и вот эти вот параметры гамма и аж

они привязаны к объектам точно нас в

нашем методе в метрическом классика

таррен наконец-то появилось некоторое

содержательное множество параметров то

есть теперь не просто там радиус

окрестности является единственным

параметром

а теперь у нас появилось два или

параметров где или то длина обучающей

выборке то есть каждый объект обучающей

выборке

оказался связанным с двумя параметрами

их можно обучать их можно настраивать

для них можно выписывать оптимизационные

задачи и здесь появляется какая-то вот

такая интересная возможность на самом

деле

поскольку теперь с объектами связаны вот

эти два параметра

нам даже нет никакого смысла нумеровать

объекты по возрастанию расстояний и

здесь можно записать просто сумму по

всем объектам обучающей выборке

и выписать вот такую вот функцию веса и

получается что объект классифицируется

по вот таким вот

взвешенном функциям близости от заметьте

что ро

это расстояние а к к утру вот это вот

уже можно назвать близостью то есть чем

важно запомнить различия в терминологии

ро расстояние чем дальше тем больше к

атра

эта близость чем дальше тем меньше то

есть эта функция убывающие от ну и

конечно возникают вопросы как задавать

эти параметры гаммы и аж как подбирать

ядро что зависит от ядра

но прежде чем перейти к этим вопросом

еще один очень важный момент и очень

важная взаимосвязь между вот этим

методом потенциальных функций до

потенциальная функция когда каждый

объект обучающей выборке распространяет

вокруг себя некий потенциал

или как я сказал запах до своего класса

вот оказывается что этот метод

тесно связан с линейным классификатором

давайте для простоты рассмотрим задачу с

двумя классами как мы уже видели общая

конструкция метрического классе катара

через ок макс в этом случае легко

записывается через знак разности значит

гамма y от x

эта оценка степени принадлежности

объекта x классу y у нас две такие

оценки за класс + 1 заказ -1 и берем их

разность но гамму мы только что написали

да то есть мы ее можем написать через

сумму объектов своего класса но когда мы

здесь написали разность двух гаммы это

все равно что мы написали сумму по всем

объектам

всех классов но объекты класса + 1 мы

учитываем с коэффициентом + 1 вот он y и

ты объекты класса -1 мы учитываем с

весом минус 1 получается так что у нас

знак суммы по всем объектам выборки

значит гаммы это эта амплитуда

потенциала к это сам потенциал

с радиусом действия ashita

и не еще надо домножить на коэффициент y

это и вот смотрим мы на эту конструкцию

и понимаем что у нас даже у

что что-то похожее мы уже видели до чем

это отличается от линейного

классификатора в линейном классификаторе

была сумма тоже был знак знак суммы по

всем признакам

с некоторыми коэффициентами f g ты это

признак объекты x и смотрите что

получается что в методе потенциальных

функций фактически в роли признаков

используется близости между объектом x и

объектами обучающей выборке вот это вот

очень важный прием который надо хорошо

запомните всегда иметь в виду при

решении практических задач вы имеете

право им пользоваться даже если у вас

уже есть какое-то признак его описание

объе

вы можете взять функцию расстояния между

их сок и фиксированным объектом

обучающей выборке любым и

сконструировать из этого новый признак

вот это вот способ в некотором смысле

фича extraction то есть если у вас есть

функция расстояние между объектами вы

всегда можете сконструировать

новая признака описание в которых

признаков будет ровно столько же сколько

было объектов обучающей выборке

вот этот вот f g ты я выделил его

красным это есть ни что иное как вот эту

вот конструкция но близости надо брать

при этом с весом плюс минус единица то

есть близость адам нажать на метку

класса на плюс-минус единиц

вот такой вот интересный прием причем

можно уже смекнуть я сейчас немножко

забегу вперед и

вспомните до что в линейном

классификаторе можно использовать

регуляризации в том числе можно

использовать регуляризации для отбора

признаков и задаться вот здесь вот

вопросом а действительно и нам нужно

мерить расстояние до всех объектов

обучающей выборке нельзя ли может быть

отобрать небольшое количество объектов и

только до них мерить расстояние и вот

эта вот аналогия с линейной

классификации если ее продолжать до то

есть понятно что признаки это функции

близости

до объектов обучающей выборке продолжим

аналогию а если мы используем например

или один регуляризация вас сода или

ластик нет для того чтобы обнулять

коэффициенты при признаках 1 обучая вот

эти коэффициенты гамму мы фактически

обучаем потенциалы

это окей а второй если мы используем

метод который может обнулить эту гамму

мы используем отбор признаков а в нашем

случае это отбор объектов обучающей

выборке то есть мы какие-то объекты

посчитаем эталонными то с теми объектами

которые являются опорными только до них

нужно считать 1

я не когда мы пытаемся классифицировать

объекты и вот это еще один очень

интересный очень важный прием который

здесь может быть использован смотрите

вот что вы сделали сейчас немножечко

ретроспективно мы начали с того что мы

определили общую формулу метрического

классификатора а потом начали

вот миду до ближайшего соседа постепенно

взвешенные ближайшие соседи парсонс как

но потом минут потенциальных функций мы

эту конструкцию постепенно обобщали и

сейчас дошли до того что поняли что

близости могут быть просто признаками и

это опять-таки просто линейный

классификатор смотрите какое мощное

обобщение линейный классе кадр этот

признак принцип раздели масти с другой

стороны у нас гипотеза компактности

принцип сходства или близости и опять мы

его довели до аналогии с линейным

классификатором и тут же линейный классе

катар это примитивный нейрон и отсюда

развитии нейронных сетей как все похожи

в машинного обучения

одно на другое между всеми методами есть

какая-то очень большая фундаментальная

общность давайте я на этом моменте при

остановлюсь и спрошу все ли понятно и не

нужно ли что-то еще объяснить если у вас

есть вопросы пожалуйста задавайте

так коллеги скажите хоть что-нибудь а то

я решу что я вся 40 минут говорил в

пустоту и у нас давно пропала связь а у

они в чате пишут я вот смотрю на чат и

там пока что ничего нет один вопросик

нами не понятно да вот вид led

да вопросик

вы показывали диаграмму с такими кривыми

где

4 конечная звездочку превращалась

квадрат добыл дело вот я с такой штукой

сталкивался когда нужно было

просуммировать

ошибки на каком-нибудь объекте на

который мы смотрим чтобы понять сильно

ли этот объект ошибочное нужно его не

фиксировать как нормально или как

ошибочный ну то есть здесь получается

тоже 2 класса только ошибочный правильно

объекта неправильно это получается можно

в любом случае использовать при любой

бинарной классификации так смотрите вот

то что видно на этой картинке никакого

отношения не имеет к ошибочным не

ошибочном давайте разберемся

здесь какая-то путаница то что показано

на этой картинке это как выглядит

функция расстояния между объектами вот

мы можем определить расстояние между

двумя объектами и здесь показана iq виде

стальная поверхность то есть

геометрическое место точек

объектов равноудалённых от центра и а

вот картинка показывает как это и креди

стальная поверхность зависит от

параметров и в метрике минковского здесь

вообще не шло речи о какой-то

классификация каких-то ошибках это

просто иллюстрация к тому как выглядят

метрики

которые мы можем определять для решения

задач пресекаться вопрос по всей

видимости о чем-то другом давайте не ним

вопрос примерно об этом просто я

получается сам использовал эту штуку для

ну то есть если объект далеко от

нормального то мы его выкидывает диском

близко к нормальному то мы его не

выкидываем ой как раз после и такую же

функцию использовал ну да иметь право в

общем евклидова метрика это только одна

из этого ряда до вокруг дашь да да

ответствует а также зависимости от того

каков смысл выходе какой смысл вы хотите

придать вашим вашей метрике и для чего

вообще используйте может быть вам

понадобится выбрать какой-то другой п ну

вот частый выбор это либо п равно 1 либо

по равно бесконечности это часто

используют ну вот варианты типа п 0 5

это такая вот немножко кривоватая

конечно метрика и и редко используют на

практике вот и кстати не берите большые

п если вдруг вы начнете

экспериментировать где-то в каких-то

задачах зададитесь целью подобрать

параметр p функции близости когда п

становится там порядка уже там 55 вам

810 все становится очень вычислить на

неустойчиво то есть это довольно такая

поганая функция с точки зрения нее

вычислений ну понятно p равно

бесконечности это просто максимум дату

его легко вычислять не нужно всех этих

радикалов считать хорошо давайте пойдем

дальше там еще кое-то вопрос получать

сейчас посмотрю

если какие-то специальные рекомендации

выборе

и драка до рекомендации есть я на них

остановлюсь когда буду говорить про

регрессию чуточку позже покажу какие

бывают ядра и тогда одновременно

вернемся к классификации и обсудим какие

там есть

эвристики и возможности с выбором я der

я сейчас хочу сказать рассказать один

такой маленький маленькая вставка

теоретическая но которая очень такие

интересные дает выводы о метрических

классификаторах и о том как можно делать

отбор эталонных объектов давайте

рассмотрим функционал паола восходящего

контроль комплект просто выделив продал

когда мы в обучающей выборке оставили

все могло и на этом одну делаем

классификацию спрашивали дышит обобщение

каждый раз оставляли этих самых

построено все появились что не слышу

проблема со звуком были чуть-чуть сейчас

вроде бы нормально здесь

и мы всеми возможными способами

разбиваем нашу выборку на обучение

контроль вообще вот комплит кросс

вылитый шин это оценка которая

определяется очень неконструктивно

особенно если к

большое вот при к-ром единицы тарифа на

двойки

время оставляет контроль то есть

величину и можно формализовать

очень простое определение давайте

назовем профилем компактности выборки

вот такую функцию очень просто которых м

той сосед лежи в другом классе ну

понятно что доля объектов у которых м

той сосед первый сосед лежит в другом

классе это просто количество ошибок

лифанов практически очевидно да вот но

мы слегка обобщаем

окрестность

двух соседей 3 соседи так далее и вот

определим такой вот профиль компактности

и есть теорема которые они будут

доказывать смысл и очень простой что вот

для этой самой величины комплит кросс в

ли дышим который очень сложно устроена

можно чисто комбинаторными методами

доказать очень простую вычислительную

формулу которая состоит из суммы по м

вот этот профиль компактности домножаем

на некую комбинаторную величину которая

обладает экспоненциально убывает

свойство специальный поэм то чем выше

м тем меньше вот этот вот множитель он

просто экспоненциально стремится к нулю

давайте посмотрим как это выглядит вот

ряд модельных задач и к 1 задачка

понятное дело для обмена до ближайшего

соседа очень простая 2 посложнее 3 еще

сложнее четвертое это где вот это вот

пила она стала совсем частая и вот

просто объекты двух разных классов

взаимно проникают друг друга и вот эта

задача она просто не решаема для meta до

ближайшего соседа во втором ряду график

отображены соответсвующее профили

компактности и вот видно что в легкой

задача профиль компактности начинаются с

нуля задача посложнее до профиль

компактности все равно своей первой

точке он 0 а вот здесь вот все гораздо

хуже а здесь все совсем плохо то есть

профиль комфортности он показывает торна

это некая функция функция от n до числа

ближайших соседей еще раз только

опасности

это доля объектов у которых м той сосед

лежит в другом классе то есть если ваши

ближайшие соседи лежат в том же классе

что и вы-то метрические методы будут

легко решать эту задачу

даже самый простой даже метод ближайшего

соседа и вот это вот как раз показано на

вот этом вот

графики где начальный участок хорошо

горизонтально принадлежит на

на нуле и это означает что это легкая

задача то есть можно вычислять профиль к

опасности по нему судить о том вообще

насколько данная задача решатель на

методом ближайших соседей или какими-то

метрическими методами ну и на нижнем

графике показан такой чисто технический

момент можно задаться поставить ли от

длинны контрольные выборки вот это вот

значение complex кроссовый леди шин и

показано что в общем нет зависимости нет

здесь некая зашумленных константа это на

самом деле не к и обоснование того что

можно пользоваться lifan out то есть вам

достаточно

один объект выделять качестве

контрольного то есть не надо мудрить

и разделять выборку на большой train

небольшой тест

если вы берете и по одному объекту в

качестве контрольного потом по нему

соединяете этого достаточно для

адекватного оценивания качества то есть

своего рода обоснования лифанов

ну в принципе да я вот сказал уже о

основных свойствах профили компактности

и теперь как это как это можно применить

как можно применить быстрого вычисления

либо ли фанат либо комплит кроссовый ли

дышим вот давайте рассмотрим такую

модельную задачку здесь взяты

1000 объектов

двух классов классы сгенерированы

из гуся он бы двигался анны и

намеренно в выборку внесено какое-то

количество шума среди черных объектов

есть островки белых и наоборот среди

белых есть островки черных и разделяющая

поверхность которую вы здесь видите это

метод

1 ближайшего соседа теперь что мы делаем

мы берем вот этот самый функционал

который мы теперь знаем как эффективно

можно вычислить вот этот комплект кросс

вылитый шин и задаемся вот какой целью

каналу оптимизировать количество

эталонных объектов которые достаточно

сохранить в обучающей выборке чтобы по

ним хорошо классифицировались все

остальные объекты и первое что твой

долго ну давайте мы возьмем там по два

объекта из обоих классов а потом

поставим такую задачу на какой третий

добавить в нашу обучающую выборку

чтобы значение функционала

было наилучшим добавили 3 добавили 4 то

есть такое жадное добавление объектов по

одному каждая каждый раз мы пируем вот

эту самую функционал ну раз мы его можем

легко вычислять эффективно то почему бы

не сделать такую оптимизация это

приводит вот такой картинки

действительно добавились какие-то

объекты

эталонные черненькие и разделяющая

поверхность стала гораздо лучше и мы

видим что гораздо меньше ошибок типе вот

если раньше было были ошибки где-то там

в районе разделяющей поверхности были

ошибки

естественно на тех объектов которые

изначально были shemale и вот а теперь

вот здесь вот

разделение гораздо более правильно

проходит но возникает вопрос а против

действительно ли можно таким жадным

алгоритмом

по одному добавлять объекты оптимально

ли это вот кажется что здесь не

оптимально потому что возникает такое

кровотечение что почему-то эти эталонные

объекты выстроились подальше от

разделяющей поверхности насколько это

правильно то есть кажется что нам

наоборот надо было бы построить границу

не

да классами и выстроить вот как

пограничной заставы у двух азбук

то друг напротив друга у границы и так

действительно происходит если мы

совершенно другую стратегию применим а

давайте мы сначала возьмем все обучающую

выборку а потом будем из нее исключать

объекты по одному но так чтобы каждый

раз снова жадный алгоритм но каждый раз

чтобы оптимизировался наш функционал

комплит кроссовый ли дальше и здесь

получается совершенно другая картинка

сначала процесс исключения выкинет из

выборки все вот эти треугольнички это

шумовые объекты это очень здорово это

означает что то такая стратегия

позволяет эффективно находить шумы в

обучающих данных а потом начнем

выкидывать всё остальное и будем

выкидывать выкидывать пока у нас не

останутся те объекты которые

действительно критически важны для

хорошей классификации

действительно кажутся пограничные

объекты то есть последовательный отсев

это более эффективная стратегия она

точнее простраивает объекты между

классами к тому же позволяет выкинуть

шумы в явном виде и вот интересный

график на эту тему по горизонтальной оси

отложено количество выкинутых объектов я

по вертикальной оси отложен комплит

кроссовый ли дышим функционал который мы

вычисляем и

видно что вот первые примерно 40 50

объектов на 40 до вот мы их выкидываем и

мы слегка улучшаем значение вон нашего

функционала причем что интересно здесь

черные линии показано ошибка на тесте на

самом деле мы вот сгенерировали не

тысячу объектов а на самом деле было

сгенерировано две тысячи объектов одним

и тем же способом и мы одну тысячу ватт

использовали для нашего эксперимента на

второе проверяем обобщающую способность

и самое удивительное что оказывается что

вот такой способ от села

не дало ответов он имеет очень хорошая

обобщающую способность то есть у нас

синхронно идут две кривые это комплект

на обучающей выборке и на отложенный

дополнительно до такого же объема 1000

объектов и мы там не видим никакого

переобучения то есть отсюда вывод отсев

и не информативных объектов вплоть до

небольшого количества эталонов эта

процедура которая не перри обучается

видимо за счет того что для в качестве

критерия мы использовали комплект

кроссовый людей шин которые

характеризуют обобщающую способность но

ты кстати в данной конкретной задачи

видно да вот я здесь вырезал основной

вообще кусок этого графика то есть

показан процесс где первые 50 объектов

были выкинуты потом вот выкидывание она

продолжалась а функционал вообще немного

где-то там 980 вот когда я начал

пробовать выкидывать критически важные

объекты последние 20 объектов то есть по

сути в этой выборки для классификации

достаточно сохранить 20 объектов на

самом деле можно еще меньше но там пару

процентов качество пожар задача но

история про то что отброс отбрасывания

шумовых от объектов и отбор эталонных

объектов это процедуры которую вот в

методе ближайшего соседа на все мне

перри обучалась это вот такой вот

интересный прием ну и давайте спада того

что мало про метрические классификаторы

значит первое вообще метрическая

классификация самое простое что есть

машинного обучения это велюр да то есть

ничего не надо делать практически ничего

оптимизировать и просто сохранить

обучающую выборку но

или поэтапно как можно усложнить эту

модель и как можно ее наделить

нетривиальными параметрами

начали с числа ближайших соседей ширины

окна посмотрели на парус как но

посмотрели на метод потенциальных

функций осознали что потенциальные

функции это на самом деле линейный

классификатор если определять признаки

через близости вот что еще можно

потенциально обучать ну вот набор

эталонов который называется по английски

prototype selection я сейчас показал

один из возможных вариантов с помощью

критерия полного скользящим контроля на

самом деле есть тема которой мы не

успеваем коснуться она достаточно

большая хорошо развита много литературы

на эту тему как обучать метрики то есть

в метриках и вот начал до взвешенная

метрикам минковского и каждому признаку

в этой метрики можно придать некий вес

можно веса просто из нормировки взять но

можно эти леса обучать и вот обучение

весов в функциях расстояниях это

отдельная такая не очень простая задача

хотя в принципе можно даже выписать

функционал который будет

оптимизироваться например не дам

стохастического градиента

коэффициента вот в этой метрики ведь

дело в том что метрика очень

нетривиально в этом алгоритме

используется она используется через

ранжирование объектов и вот это вот

функция ранжирования объектов по

возрастанию расстояний она она не

дифференцирует приятно но если вы

посмотрите на метод давайте я отмотаю

назад

метод потенциальной функций смотрите

здесь очень интересный прием мы сделали

мы ввели весах а таким образом беса

этого соседа что нам больше не нужна

ранжировать соседей нам больше не нужна

упорядочивать объекты по возрастанию

расстояний потому что вот такой функции

мы уже можем в любом порядке

записывать объекты обучающей выборке в

нашем функционале и вот этот функционал

он уже гораздо приятнее зависит от

метрики вот в этот функционал уже можно

впихнуть метрику которая зависит от

каких-то параметров и дальше просто как

вот в линейных классификаторах надо

выписать

некую функцию от моржана функцию потери

просуммировать эти потери записать мир и

дальше у минимизировать стохастическим

градиентом то есть вот один из подходов

к тому как можно сделать distance

learning

distance learning или 7 лет теленок вот

ну и последнее о чем я еще не рассказал

как оптимизировать

функций утра вообще нужно ли и

оптимизировать это мы сейчас обсудим

когда будем разговаривать про регрессе

все ли понятно здесь было давайте я при

остановлюсь на минутку и обсудим так это

вопрос сейчас да это число сочетание

конечно

так если еще какие то вопросы если дне

давайте пойдем дальше у нас с вами

осталось регрессия давайте обобщать и то

что мы придумали с функциями расстояния

или близости вот давай теперь попробуем

посмотреть как это можно использовать

для решения регрессионных задач идея та

же самая близким объектом должны

соответствует близкие ответы и можно ли

в регрессии тоже применить что-то вроде

методы ближайшего соседа и так у нас

есть обучающая выборка но теперь ответы

на объектах это не элементы конечного

множества а это элементы действительной

оси

точки 10 классе вот она наша неизвестная

зависимость и мы хотели бы приблизить ее

как мы на первой лекции написали но

некой параметрической моделью

зависимости с каким там вектором

параметров модели и давайте напомню что

есть у нас замечательная универсальный

достаточно метод для решения таких задач

от метод наименьших квадратов

вам нашу зависимость

искомого не из неизвестный параметр

выписываем функцию потерях давайте

возьмем квадратично и минимизируем сумму

квадратов невязок

о модели для общности сейчас я здесь

записал еще и некий весовой коэффициент

степень важности объекта значит всё

не подхода том что она заставляет

написать вот эту параметрическая модель

зависимости сложное и настолько мы не

знаем законов природы вот в той задачи

которые пытаемся решать что ну ну

низкого потолка мы не можем взять вот

эту параметрическое семейство функций

использовать например

линейные функции когда мы но точно знаем

что зависимость какая-то более сложное

это плохая идея но использовать вместо

линейных функций например полиномы но

это еще более плохая идея потому что

вычислительно полином очень неудобная

штуковина

ну и возникает вопрос вот есть класс

задач где но никак не удается выписать

хорошую параметрическая модель можно ли

обойтись вообще без параметрических

моделей вот это мотивация привела к

созданию широкого класса методов которые

называются

не параметрическими как раз основан на

идее близости между объектами подход

следующий давайте перенесем всю

сложность решения задачи и за проблемы

создания модели по метрической продаем

определения расстояния между объектами

как это делается на первое мы говорим

пусть наша модель будет максимально

простой какая только возможна самая

простая модель это константа

то есть мы будем приближать нашу искомую

функцию константой но окей это как бы не

очень интересно с константа посчитано я

по всей обучающей выборке ну это

тривиальная вырожденная модель которая

вряд ли кому

может быть интересно но тут возникает

вторая идея а давайте мы эту константу

будем определять не по всей выборки а по

локальной окрестности того объекта x

которым мы нашу зависимость хотим

посчитать и вот тут вот и возникает

возможность использования функции

расстояния между объектами давайте

определим веса объектов методе

наименьших квадратов

теперь не как постоянную как-никак

константы

а как функцию от того объекта x в

котором мы хотим посчитать

нашу регрессионную модель и определим

значение лесов ну вот просто точно так

же как мы это делали для метрических

классификаторов мы возьмем функцию

расстояния мы возьмем какое-то ядро

функцию расстояния поделим на ширину

окна

она теперь называется окно сглаживание

почему сглаживание то что мы в каждой

точке x пытаемся разгладить значение

ближайших объектов

ближайших из окрестностей радиуса аж

этого объекта то есть как бы для того

чтобы построить

зависимость мы телефон ты мне всю

обучающую выборку а только окрестность

радиуса аж нашего объекта

ну и поскольку модель зависимости теперь

у нас очень простая это альфа альфа это

единственный параметр вот этой модели а

давайте мы найдем аналитическое решение

по альфа и она ищет очень просто я здесь

даже не стал потому что она в одну

строчку делается его можно даже сделать

в уме

давайте вот этот вот функционал

продифференцируем по альфа и приравняем

нулю производной

что у нас получится вместо этой скобочки

но получится 2 на альфа минус y это

два показатель степень значит 22

уйдет при приравнивание нулю производной

и у нас тут же появится вот такая вот

формула которая называется формулой надо

рая ватсона или формулы ядерного

сглаживания то есть что это за формула в

чем ее смысл это средневзвешенное

значение

ответов на объектах обучающей выборке

но эти ответы взяты с весами которые

зависят от объекта и чем дальше от

нашего объекта x тем меньше вот эти

весовые коэффициенты

представьте себе что эти весовые

коэффициенты были бы одинаковыми на всей

выборки чтобы мы получили но вы получили

просто среднее арифметическое значение

всех игроков по всей обучающей выборке

то есть дубль во это равно единичке да

здесь суммы игреков а здесь el dar

среднеарифметическая получили бы ну и на

самом деле среднее арифметическое

это точное решение задачи наименьших

квадратов

если модели является константа ну а если

использованные веса объектов то тогда не

среднеарифметическая средневзвешенная

вот ну и вот vesa записали в виде ядра

от функция расстояние между объектами

вот это вот формула называется формулой

удара и ватсона очень часто используются

для сглаживания сигналов то есть она

очень простая вычисления в объекте x

занимает время порядка длины обучающей

выборке но на самом деле можно это время

сократить если использовать так

называемый финишный я так вот есть еще

одна теорема которая позже привожу без

доказательства и она весьма

нетривиальная и я даже не буду разбирать

здесь вот все эти закорючки которые

приведены но смысл этой теоремы скат ну

и смысл под чьим это обоснование формула

надорваться на вот мы ее сейчас вывели

из метода наименьших квадратов

но возникает конечно вопрос а будет ли

вообще вот это вот формула

аппроксимировать на

зависимость каком смысле и как это будет

зависеть от ширины окна сглаживания и от

ядра и что говорит это теорема

первое она говорит что если наша выборка

сгенерировано нет и неким распределением

совместном иксов игреков то мы тогда

можем претендовать на то чтобы

восстанавливать вот такое вот

математическое ожидание посмотрим вниз

теорема что оно утверждает она

утверждает что формула нодара и ватсона

даст не смещенную оценку для

математического ожидания игрека при

условии x то есть это и есть наша

восстанавливаемой а зависимость с

точностью до шума означает что та самая

зависимость который мы восстанавливает

теорема утверждает что мы действительно

будем сходиться именно к тому чему нужно

при каких условиях 1 что вы вообще имеет

отношения к восстанавливаемой

зависимости очевидно что это потребовать

второе ядро ограничено и то есть ядро

она в общем не обязана быть убывающей

функцией вот прям вот буквально но на

бесконечность она должна стремиться к

нулю это свойство локальности ядра ядро

должно взвешивать действительно те

объекты которые находятся рядом с точкой

тоже естественная требует третье мы не

вправе претендовать на какую-то точность

установления нашей зависимости если

зависимость имеет вертикальную асимптоту

ну это тоже очень разумно то есть

поскольку idroid такая локальная

окрестность объекта x

если в этом объекте есть вертикальная

асимптота ни на что нельзя претендовать

мы не сможем восстановить

в такой точке значения нашей зависимости

тоже вроде разумное ограничение и на

самом деле никакой это не ограничение

если мы восстанавливаем функция

непрерывна то у нас вообще все хорошо но

и четвертое очень важное для понимания

условия чтобы восстанавливать хорошо

восстанавливать зависимость нам нужно

чтобы ширина окна сглаживания с ростом

длины выборки убывала но не слишком

быстро то есть она должна убивать до 0

но вот такой ряд должен быть то есть она

не должна бывать быстрее чем функция

единица поделить на и вот но например

там единиц поделить на корень из эль нам

подходит вот мы тогда все хорошо

на самом деле таких обоснования очень

много я приложу лишь простейшую теорему

которая для одномерного случая работает

если x это это действительные числа рта

вот работает эта теорема если это

например вектор р н то есть аналогичная

теорема но она гораздо больше

технических требований теперь про я др

наконец давайте обсудим какими бывают

ядра на самом деле и в классификации

игрогрехи а потом мы с вами через

коллекция будем уже говорить про

восстановление плотности распределения

по выборке и там возникнет

опять-таки очень похожая конструкция

ядро от функции расстояния вот во всех

этих задачах и классификации и регрессии

установили ее плотности распределения

можно использовать

одни и те же ядра из распространения

часто использую показаны вот на этой

картинке

вот ядро прямоугольная ядро треугольная

прямоугольное ядро она имеет два разрыва

это не очень приятно и если вы

посмотрите на то как устроена формул

андорра его цена очень быстро поймете

что гладкость той зависимости которую мы

получаем она связана с гладкостью ядра

если ядро непрерывно дифференцируемая то

и функция которую мы построим тоже будет

непрерывно дифференцируема если

постоянно то и наша функция тоже будет

кусочно постоянно то есть вывод здесь

такой что но давайте посмотрим да как на

примере посмотрим как ведет себя

аппроксимирующей а функция

если мы будем брать разные игры и разную

ширину окна вот смотрите взяли

гангстерской ядро она чем хорошо она

гладкая то есть она бесконечная

бесконечно дифференцируемы и функция

которую мы строим она тоже гладенькая и

мы это видим на примере но подать

посмотрим чем различаются три кривые при

разном значения ширины окна если мы

слишком маленькую возьмем ширину окна мы

начинаем гоняться за каждыми шумами за

каждым шум за каждой точкой и пределе

пряжу стремящемся к нулю мы просто

построим кривую которая проходит через

заданные точки но если эти точки нам

загоны с какими-то шумами то это совсем

не то что нам нужно если мы возьмем аж

слишком большим там и перри сглаживаем и

если h стремится к плюс бесконечности

там и в итоге будем стремиться константе

напомните тривиальное решение задачи

наименьших квадратов как дар зависимость

от константа да это среднее

арифметическое то есть мы будем

стремиться к выраженному случаю есть она

всегда посередине и качество

аппроксимации очень сильно зависит от

ширины of разглаживания вот это важно

важный параметр который важно

оптимизировать

давайте возьмем другое ядро вот здесь мы

взяли ядро треугольно

это ядро непрерывная но не гладкая и что

важно ядром fi нет на и

то есть в окрестности от нуля до единицы

она не 0 а дальше 0 и что же мы видим

что если ширина окна слишком маленько

то у нас наш аппроксимирующей функции

внезапно образуется вообще разрывы вот

что такое разрыв

там значение аппроксимирующей функции не

образина почему потому что формуле на

дара и ватсона

давайте мне посмотрим ещё раз вот она

формула нодара его цена вот здесь стоит

ядро

а представьте себе и дров и нет на я и в

окрестности объекта x не нашлось ни

одного объекта

nostra получится ноль делить на ноль то

есть у вас в числителе 0 слагаемых и в

знаменателе 0 слагаемых и вы просто не

можете определить значение этой функции

вот таких разрывов когда ядро было

экспоненциальном не было потому что эта

функция не fi нет на я для любого р

сколь угодно большого существуют

ненулевое значение и поэтому здесь все

прекрасно оу здесь образовались

интервалы на которых в которых ни один

объект

обучающей выборке не попал в столь

маленькое окно

но заметьте еще даже в чем различия

смотрите сейчас поселку два слайда

гауссовское ядро треугольное ядро мы

видим что на гладкость функция ядро

влияет сильно да то есть какого

гладкость ядра

такого гладкость аппроксимирующей

функции но сравните вот эти черные

кривые то есть это оптимальная ширина

окна но от вида ядра

вид этой аппроксимирующей функции почти

не зависит

аппроксимирующей а функция сильно

зависит от ширины окна но почти не

зависит от вида

ядра и это очень интересное свойство

давать возьмем прямоугольное ядро что мы

увидим мы увидим конечно же кусочно

постоянную функцию и дров и нет на и

поэтому если ширина окна слишком

маленькой мы увидим разрывы и общем

график уже такой достаточно неприятной

но смотрите несмотря на то что эта

функция ступенчатая она все равно черную

кривую смотрите при оптимальный ширине

окна

эта функция все равно является неплохой

аппроксимации то есть опять-таки

качество прокси maciej существенно

зависит от ширины окна но не существенно

зависит от вида ядра как выбирать ядро

ширину окна ширина окна важный параметр

который можно подобрать по лифанов

то есть тот же рецепт который был в

методе ближайших соседей для

классификации такой же рецепт для

подбора ширины окна и для регрессии для

формы лондора и ватсона

здесь так же можно использовать

переменную ширину окна если

подбирать ее по расстоянию до pack + 1

соседа я правда при этом теряется

гладкость функции поскольку у нас к + 1

сосед может меняться скачком при малом

изменении ксор существуют границы где с

одного объекта на другой мы

перескакиваем в этом месте может

нарушаться гладкость хотя непрерывность

останется вот поэтому вот

переменной шириной окна пока + 1 соседу

пользоваться надо осторожно чем можно

пользоваться какими-то кадрами которые

вот как гауссовское ядро она не винит на

и она

всюду непрерыв дифференцируема и поэтому

приятно пользоваться но зато вы

вынуждены учитывать все обучающую

выборку как окрестность любого объекта x

так и последний момент

что еще хотелось здесь обсудить связи с

формулой нодара и ватсона это

устойчивость не устойчивость по

отношению к выбросам снова модельная

выборка и теперь я сделал две таких

пакости я добавил два объекта на которых

сильно искажён правильный ответ то есть

допустим в обучающую выборку затесались

объекты на которых по каким-то нелепым

случайностям сильно-сильно неправильные

ответы

видите диапазон значения от минус двух с

половиной до четырех и вот на двух

объектах вот на этом этого объекта не

видно здесь на этой картинке но это .

есть у нее значение плюс 40 и есть ещё

одна . у нее значение минус 40 что

произойдет аппроксимирующей функции если

у нас выборке есть вот такие выбросы

вот произойдет то что вы видите да

красная кривая это аппроксимирующей

функция когда выборке есть вот эти два

выброса если эти выбросы удалить то

получится нормально хорошие proximo ции

которая нарисована синим и вопрос но

хорошо глазами мы можем увидеть эти

выбросы только когда задача одномерно

тогда мы можем нарисовать

как зависит y от x а на графике увидеть

все точки ну и посмотреть хорошо это или

плохо представьте себе ситуацию когда у

вас x и

это объекты в каком-нибудь тысячи мерном

пространстве и тогда у вас практически

не остается никакого такого визуального

способы контроля а если в выборке

выбросы и надо как научить

автоматических обнаруживать

есть метод очень несложный который

позволяет эти выбросы обнаруживать и так

выбросы это точки у которых сильные

искажения y вид их давайте заметим что

чем больше величина ошибки а что такое

ошибка это если мы делаем lifan out в

точке x это по формуле вас на протяни

ручку если объект не влезла на график и

вот если в этой точке мы посчитаем вот

эту разность до эта точка выкинуто тогда

у нас синяя кривая это . есть выбор

тогда у нас красное пиво от мы можем

считать такие разности и погиб от арцах

идентифицировать что является выбросом

правильное значение

значение из выборки минус и еще раз да

вот значение выборки в обучающей вот для

этой точки выброса она очень большая но

плюс 40 а значение планирующей функции

она где-то в районе нуля мы должны

обнаружить по этой большой разности

что . x это является выбросом для этого

мы считаем ошибки и полив теперь

что мы можем сделать если мы видим что

вот эта ошибка большая то мы должны

уменьшить вес этой точки в формуле надо

радоваться на вот в этом идея метода

сделать какой-то коэффициент гаммы это

вот у нас будет для каждого этого

объекта обучающей выборке гамма это как

оценка того что объект является выбросом

то есть выбросом мы

собираемся эти гаммы понизить до нуля то

чем больше объект похож на выброс тем

ближе к нулю

должен быть коэффициент гамма то есть

этот объект не будет учитываться в сумах

формуле она гора его цена вот ну как

превратить большую ошибку в маленькие

близкие нулю гамма давайте возьмем снова

ядро но на этот раз это ядро она связана

с ошибками они с иксами вот поэтому это

какой-то совсем другое ядро может быть

поэтому обозначим локос волной какая-то

убывающая функция от ошибки и

рекомендация брать кварте ческое ядро да

и не сказал давайте посмотрим на то что

такое классическое ядро у нас были ядра

что такое кварте ческое ядро вот она да

это кусочек полиномы четвертой степени q

вот она хорошо тем что она affinita я им

при этом гладкая поскольку это кусочек

полинома

у него вот здесь вот в нуле вот кривая

которая фиолетовая такая на час точках

минус 1 и плюс 1 она гладкая и ну похоже

на парабола но кусок полина

вот такой вот ядро рекомендуется брать и

ширину окна брать

почему-то как 6 медиан начинай просто

рекомендации и статьи

вот этот метод и сейчас я просто этот

метод покажу как он выглядит до

называется он лови слока ли вы этот

scatter plot смузи

почему scatter plot потому что вот этот

график которые я показывал на примерах

его обычно называется катар плод как и

игреки зависит от иксов почему смузи уж

мы занимаемся здесь

задачи сглаживания и почему локальный

этот потому что мы в процессе решения

задачи сейчас будем модифицировать vesa

зависимости от того похож объект на

выброс или нет но теперь это не просто

формула ватсона теперь это итерационный

прав

и на каждой итерации мы каждого объекта

обучающей выборке вычисляем lifan out и

вычисляем ошибку и по этой ошибки

наливка на пути да то есть а это

это на этом объекте полученная с помощью

лифанов

и дальше с помощью второго ядра к

молодой мы получаем коэффициент гамма

это и если у нас ошибки действительно

были очень большими и дрова выбор на

достаточно удачно то вот это гаммы т он

либо станет равным нулю

либо очень близким к нулю и уже на

следующей итерации вот этот дома он

фактически исключит объекты выбросы и

вот из этих сумм благодаря тому что

здесь стоят эти гаммы у нас выбросы

перестанут играть роль в обучающей

выборке

что у нас происходит вот картинка вот на

первой итерации точнее нулевое

приближение когда мы еще не знали какие

объекты выбросы каких сходу построили

выборку где вот у нас появилось два вот

этих вот

наших выброса уже на первой итерации вот

после того как мы увидели эти выбросы

посчитали гамма второй раз считаем по

формуле нодара его цена у нас уже

получается кривая очень близко к

оптимума вот это вот красненькая кривая

потом третья итерация жирная красная ну

и видно что уже процесс практически

сошелся то есть если вас использоваться

для борьбы с большими редкими выбросами

то он сходится буквально на

второй-третьей итерации то есть очень

такой эффективный метод для обнаружения

выбросов его можно брать на вооружение

на самом деле вот не только в этой

задаче

регрессия не только при использовании

формула надо радоваться на

вообще это некий подход к анализу ошибок

аппроксимации задачах регрессии возьмите

постройте вашу регрессию посмотрите на

каких объектах ошибки получили самые

большие и потом эти объекты либо вообще

выкиньте из его бы

обучающей выборке либо возьмите их с

лисами но так чтобы весаху бывали с

ростом ошибки и вот для этого убывания

можно задать некое ядро

а можно делать гораздо грубее да можно

просто эти объекты выкинуть вот это

такой простой подход к построению

робастные агрессия опасная вообще в

статистике и в машинном обучении это

методы которые устойчивы к выбросам

а в более общем случае устойчивы к

искажению тех предположений

вероятностных при которых строится

модель зависимости то есть отсутствие

выбросов это вероятностное предположение

о том что шум и как-то вот приятно

устроены да у них нет большой дисперсии

они например гауссовские с нулевым от

ожиданиям сейчас не буду в эти

статистические детали уходить но главное

что вы должны выяснить здесь ловишь это

очень простой вариант

опасного метода который позволяет из

обучающей выборке выкинуть выбросы и

смотрите то же самое мы видели когда

занимались отбором прототипов в методе

классификации первым делом что сделал

метод отбора прототипов он выкинул

выбросы то есть объекты которые лежали в

толще чужого класса они вытянулись

первую очередь примерно тот же самый

поход ног регрессии да мы смотрим на

каких объектах самые большие ошибки мы

выкидываем из обучающей выборке и решаем

задачу второй раз собственно самый

простой способ избавляться от выбросов

мы дошли до конца я уже наверно не

буду повторяться спрошу

что был непонятные какие есть вопросы

сейчас посмотрите

ага действительно вдаваться вот в эти

вот

подробности что такое сходимость по

вероятности в некотором разумном смысле

аппроксимирующей а функция приближается

к математическому ожиданию по точечно на

всем пространстве объектов это означает

что вероятность больших отклонений

стремится к нулю

можно так сказать но я не хочу здесь

вдаваться в подробности потому что в

математической статистики по поводу вот

такого сорта результатов есть отдельная

большая наука

ну и рассказываете во всех подробностях

это означает прочитать отдельный

семестровый курс

поэтому я здесь эту теорему привожу

только для того чтобы вы поняли

интерпретацию вот этих 4 условия и

теорема

чтобы с формулой надорваться на все было

хорошо нужно чтобы вы восстанавливали не

слишком плохую функцию нужно чтобы ядро

убывала и чтобы а ширина окна стремилась

к нулю с ростом выборки все то есть вот

это было сделано ради обоснования вот

этих вот ну по сути дела 3 предположений

так если еще вопросы друзья

так андрей я тогда тебе передаю бразды

правления ты дальше рули и семинаром

хорошо спасибо

